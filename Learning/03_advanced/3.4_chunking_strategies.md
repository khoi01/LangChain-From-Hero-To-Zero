# 3.4 Advanced Chunking Strategies

### Introduction
The "Chunking" phase is the most underrated part of the RAG pipeline. If your chunks are too small, the LLM loses context; if they are too large, they won't fit in the prompt or will dilute the relevant info.

### Objective
Master the "Recursive Character Splitting" technique and understand why chunk overlap is the "glue" that holds your RAG system together.

### Concept
- **Chunk Size:** The maximum number of characters (or tokens) in one snippet.
- **Chunk Overlap:** A small portion of text that is shared between consecutive chunks. This ensures that a sentence split in half still has its context preserved in both chunks.
- **Recursive Splitting:** A smart way to split text that prioritizes keeping paragraphs and sentences together before resorting to splitting at spaces or characters.

### Example
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# The standard splitting strategy for production
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", "!", "?", " ", ""]
)

texts = text_splitter.split_text(large_document_string)
```

### 2 Use Cases
1. **Legal Document Processing:** Using a larger chunk size to ensure that an entire contract clause is kept together for analysis.
2. **Code Snippets:** Using a specific "Code Splitter" that respects function and class boundaries rather than just character counts.

### Real-World RAG & Agentic Context
In **Real-World RAG**, "Basic Chunking" often fails. Advanced developers use **"Parent Document Retrieval"**. This is where you search using small, dense chunks (for accuracy) but then retrieve the *larger parent chunk* (for context) to give to the LLM. This provides the best of both worlds: precise search and rich context.
In **Agentic Systems**, chunking is about **"Information Density"**. When an agent reads "Page 4 of the manual," it needs to find the *exact* instruction. Poor chunking leads to the agent saying, "I see something about a hardware reset, but the actual steps are missing," because the steps were in the next chunk that didn't get retrieved.

### Did you know?
There is a new technique called **"Semantic Chunking"**. Instead of splitting by characters, it uses an LLM to look at the text and say, "The topic changed here, so let's start a new chunk." This is much more accurate but also more expensive to run.
