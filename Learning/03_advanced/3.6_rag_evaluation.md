# 3.6 Observability & Evaluation

### Introduction
The most common question in AI is: "Is my bot better today than it was yesterday?" Without evaluation, you are just guessing. In the real world, we use "LLM-as-a-Judge" to objectively grade our RAG systems.

### Objective
Learn the three core metrics of RAG evaluation: Faithfulness, Answer Relevance, and Context Precision.

### Concept
Evaluation is about **Automated Quality Control**:
- **Faithfulness:** Does the answer contradict the context? (The Anti-Hallucination metric).
- **Answer Relevance:** Did the LLM actually address what the user asked?
- **Context Precision:** Was the retrieved document actually the right one for this query?
By calculating these scores (0 to 1), you can see exactly where your RAG system is failing.

### Example
```python
# A conceptual 'Judge' prompt
judge_template = """
### CONTEXT: {context}
### ANSWER: {answer}
Grade the answer based on the context. 
If the answer uses info NOT in the context, score 0. 
If it is perfectly grounded, score 1.
Score: """

# In production, we use libraries like RAGAS or Arize Phoenix to automate this.
```

### 2 Use Cases
1. **CI/CD for LLMs:** Automatically running 100 test questions against your RAG system before you deploy a code change to ensure "Answer Quality" hasn't dropped.
2. **Identifying Knowledge Gaps:** If your "Context Precision" is consistently low for certain topics, it means you need better documents or better chunking for that specific area.

### Real-World RAG & Agentic Context
In **Real-World RAG**, evaluation is the **"Hallucination Defense"**. You cannot manually check every chat message. By running an automated "Faithfulness" check on every message, you can flag bad responses in real-time before they reached the customer.
In **Agentic Systems**, evaluation is about **"Trajectory Analysis"**. We don't just grade the final answer; we grade the steps. Did the agent pick the right tool? Was the tool output parsed correctly? Evaluating the "intermediate thoughts" of an agent is how you prevent it from going off the rails.

### Did you know?
The industry standard library for this is **RAGAS**. It uses an LLM (like GPT-4o) to "judge" a cheaper model's output. This creates a "Teacher-Student" dynamic where your smartest model ensures your production model is behaving.
