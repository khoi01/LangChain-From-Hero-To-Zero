# 3.6 Observability & Evaluation

### Introduction
The most common question in AI is: "Is my bot better today than it was yesterday?" Without evaluation, you are just guessing. In the real world, we use "LLM-as-a-Judge" to objectively grade our RAG systems.

### Objective
Learn the three core metrics of RAG evaluation: Faithfulness, Answer Relevance, and Context Precision.

### Concept
Evaluation is about **Automated Quality Control**:
- **Faithfulness:** Does the answer contradict the context? (The Anti-Hallucination metric).
- **Answer Relevance:** Did the LLM actually address what the user asked?
- **Context Precision:** Was the retrieved document actually the right one for this query?
By calculating these scores (0 to 1), you can see exactly where your RAG system is failing.

### Complete Implementation
Here is a minimalist "LLM-as-a-Judge" script to grade a RAG response.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1. The Context & Answer to evaluate
context = "The CEO of the company is Alice."
answer = "The CEO is Bob."

# 2. The Judge Prompt
template = """
### CONTEXT: {context}
### ANSWER: {answer}
Is the answer faithful to the context? 
Return 1 for YES or 0 for NO.
Score: """
prompt = ChatPromptTemplate.from_template(template)

# 3. The Judge Brain
llm = ChatOpenAI(temperature=0)
judge_chain = prompt | llm

# 4. Execution
# Bob is NOT Alice, so the judge should return 0
result = judge_chain.invoke({"context": context, "answer": answer})
print(f"Faithfulness Score: {result.content}")
```

### 2 Use Cases
1. **CI/CD for LLMs:** Automatically running 100 test questions against your RAG system before you deploy a code change to ensure "Answer Quality" hasn't dropped.
2. **Identifying Knowledge Gaps:** If your "Context Precision" is consistently low for certain topics, it means you need better documents or better chunking for that specific area.

### Real-World RAG & Agentic Context
In **Real-World RAG**, evaluation is the **"Hallucination Defense"**. You cannot manually check every chat message. By running an automated "Faithfulness" check on every message, you can flag bad responses in real-time before they reached the customer.
In **Agentic Systems**, evaluation is about **"Trajectory Analysis"**. We don't just grade the final answer; we grade the steps. Did the agent pick the right tool? Was the tool output parsed correctly? Evaluating the "intermediate thoughts" of an agent is how you prevent it from going off the rails.

### Did you know?
The industry standard library for this is **RAGAS**. It uses an LLM (like GPT-4o) to "judge" a cheaper model's output. This creates a "Teacher-Student" dynamic where your smartest model ensures your production model is behaving.
