# 1.4 Ollama (Local Development)

### Introduction
Running LLMs locally is the "Holy Grail" of AI development. It ensures 100% data privacy and $0 token costs. Ollama makes this possible by providing a simple way to run models like Llama 3 or Phi-3 on your own hardware.

### Objective
Learn how to connect LangChain to a local Ollama instance and understand the "Local-First" development workflow.

### Concept
1. **Local Inference:** The model runs on your CPU/GPU, not a remote server.
2. **Persistence:** Your data never leaves your infrastructure.
3. **Switching Models:** Ollama allows you to "pull" different models effortlessly.

### Example (For Docker Environment)
In this Docker environment, we connect using the service name `ollama` and the specific model we pulled: `llama3.2`.

```python
import os
from langchain_ollama import ChatOllama
from dotenv import load_dotenv

load_dotenv()

# We use the internal Docker URL and the model we downloaded
llm = ChatOllama(
    model=os.getenv("OLLAMA_MODEL", "llama3.2"),
    base_url=os.getenv("OLLAMA_BASE_URL", "http://ollama:11434"),
    temperature=0
)

response = llm.invoke("What is local AI development?")
print(response.content)
```

### 2 Use Cases
1. **Confidential Analysis:** Analyzing internal company financial spreadsheets without sending data to any 3rd-party provider.
2. **Offline Development:** Building and testing your AI application logic while on a plane or in an area with a poor internet connection.

### Real-World RAG & Agentic Context
In **Real-World RAG**, local LLMs are often used for **"Sensitive Pre-processing"**. You might use a local model to summarize or redact personally identifiable information (PII) before sending only the "clean" data to a larger model like GPT-4. This provides a layers of security called "Data Residency".
In **Agentic Systems**, local models are excellent **"Specialist Workers"**. You can have a heavy-duty model (GPT-4) as the "Manager" and use a fleet of local Llama-3 models to perform thousands of cheap, small tasks like formatting text or checking for basic errors.

### Did you know?
When running inside Docker, you cannot use `localhost` to talk to other containers. You must use the service name (e.g., `http://ollama:11434`). This is a common "gotcha" that distinguishes professional containerized AI apps from simple local scripts.
