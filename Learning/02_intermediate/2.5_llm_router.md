# 2.5 Provider Routing (Dynamic Selection)

### Introduction
The "One Size Fits All" model is dead in production. Effective AI systems use small, fast models for easy tasks and large, smart models for complex ones. Routing is the "brain" that makes this choice.

### Objective
Learn how to build a "Routing Chain" that analyzes user intent and picks the best execution path.

### Concept
Routing is a **Logical Fork** in your pipeline:
1. **Classifier:** A small model or logic determines the "category" of the input.
2. **Router:** A mapping that sends the input to the specific chain for that category.
3. **Execution:** Only the chosen path is run, saving time and tokens.

---

### Step-by-Step Implementation

#### 1. Setup Models and Parser
We initialize two models: a high-performance one (**GPT-4o**) for complex tasks and a local one (**Llama 3.2**) for general chat.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama

# 1. Models
llm_complex = ChatOpenAI(model="gpt-4o", temperature=0)
llm_slow = ChatOllama(model="llama3.2", temperature=0, base_url="http://ollama:11434")

parser = StrOutputParser()
```

#### 2. Define Specialized Prompts
Each path needs a specific "personality". We create one for coding and one for general assistance.

```python
# 2. Prompts
prompt_code = ChatPromptTemplate.from_messages([
    ("system", "You are a senior software engineer. Answer with code-focused explanations."),
    ("user", "{input}")
])

prompt_general = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant for general questions."),
    ("user", "{input}")
])

# 3. Chains
coding_chain = prompt_code | llm_complex | parser
chat_chain = prompt_general | llm_slow | parser
```

#### 3. Pre-processing (Intent Extraction)
We normalize the input (e.g., lowercase the topic) to ensure our routing logic is robust.

```python
# 4. Intent extractor
def extract_intent(x: dict) -> dict:
    topic = x.get("topic", "").lower()
    return {
        "topic": topic,
        "input": x["input"]
    }

intent_extractor = RunnableLambda(extract_intent)
```

#### 4. The Router Logic
`RunnableBranch` acts like an `if-else` statement for your chains.

```python
# 5. Router (RunnableBranch)
branch = RunnableBranch(
    (
        lambda x: any(k in x["topic"] for k in ["code", "python", "bug", "error"]),
        coding_chain
    ),
    chat_chain  # Default fallback
)
```

#### 5. Build and Run
We connect the extractor to the router using the `|` operator.

```python
# 6. Full pipeline
full_pipeline = intent_extractor | branch

# 7. Invoke
result = full_pipeline.invoke({
    "topic": "Python code debugging",
    "input": "Why does my list comprehension throw an IndexError?"
})

print(result)
```

### 2 Use Cases
1. **Technical vs. General Support:** Routing hardware-specific error codes to a deep technical RAG, while general "How do I return a product?" goes to a standard FAQ chain.
2. **Language-Specific Pipelines:** Routing Spanish queries to a Spanish-indexed vector store and English queries to an English one.

### Real-World RAG & Agentic Context
In **Real-World RAG**, routing is the key to **"Multi-Index RAG"**. You might have one index for "Product Manuals" and another for "Legal Contracts". A router ensures the LLM doesn't look for a warranty in the legal section, improving search accuracy and reducing noise.
In **Agentic Systems**, routing IS **"Agent Delegation"**. In a multi-agent system (like LangGraph), a "Supervisory Agent" routes tasks to specialized "Worker Agents" (e.g., a "Coder Agent" or a "Researcher Agent"). This is the pinnacle of modern AI architectureâ€”specialization over generalization.

### Did you know?
Routing can be done without an LLM! If you have a list of keywords or a simple regex, you can route using standard Python functions injected into an LCEL chain with `RunnableLambda`. This is faster and 100% free.
