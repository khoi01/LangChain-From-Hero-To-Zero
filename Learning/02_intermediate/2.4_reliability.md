# 2.4 Error Handling & Reliability

### Introduction
In the real world, APIs timeout, models hallucinate, and user inputs are messy. A reliable AI system is one built on the assumption that anything can fail.

### Objective
Master the patterns of "Retries" and "Fallbacks" to build systems that stay online even when the provider goes down.

### Concept
Reliability in LangChain is about **Chain Resiliency**:
- **Exponential Backoff:** Retrying an API call but waiting longer each time to avoid being banned for rate limits.
- **Model Fallbacks:** Automatically switching to a different provider (e.g., OpenAI -> Azure -> Ollama) if the primary one returns an error.
- **Output Validation Fallbacks:** If the model returns invalid JSON, retrying with a different prompt that says "You made a mistake, please try again."

### Example
```python

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain.callbacks.base import BaseCallbackHandler

#Tracking with LLm is applied
class ModelTrackingCallback(BaseCallbackHandler):
    def __init__(self):
        self.model_name = None

    def on_chat_model_start(self, serialized, messages, **kwargs):
        self.model_name = serialized.get("name") or serialized.get("model")
        
# -------------------------------------------------
# 1. Prompt (RAG / Legal safe)
# -------------------------------------------------
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a Legal Assistant.
Use ONLY the provided DOCUMENT CONTEXT.
If the answer is not in the context, say:
"I cannot find this in our database."
Do NOT use external knowledge."""),
    ("user", "DOCUMENT CONTEXT:\n{data_context}\n\nQUESTION: {question}")
])

# -------------------------------------------------
# 2. Models (ordered by preference)
# -------------------------------------------------

# Primary (best quality)
gpt4 = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
    timeout=30
)

# Fast / cheaper fallback
fast_gpt = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    timeout=20
)

# Local offline fallback (Ollama)
local_llama = ChatOllama(
    model="llama3.2",
    temperature=0,
    base_url="http://ollama:11434"  # Docker service name
)

# -------------------------------------------------
# 3. Robust model with fallbacks
# -------------------------------------------------
robust_model = local_llama.with_fallbacks([
    fast_gpt,
    gpt4
])

tracker = ModelTrackingCallback()


# -------------------------------------------------
# 4. Output parser
# -------------------------------------------------
parser = StrOutputParser()

# -------------------------------------------------
# 5. LCEL Chain
# -------------------------------------------------
chain = prompt | robust_model | parser

# -------------------------------------------------
# 6. Invoke
# -------------------------------------------------
result = chain.invoke({
    "data_context": "Doc 42: Employees are entitled to 20 days of annual leave.",
    "question": "How many days of annual leave do employees get?"
}    ,config={"callbacks": [tracker]}
)

print(result)
print("Model used:", tracker.model_name)

```
 
  
   
    
     

### 2 Use Cases
1. **Critical Service Hubs:** A customer support bot that must answer simple queries using a local model if the internet connection is lost.
2. **Cost-Aware Systems:** Attempting a task with a cheap model first and "falling back" to a smarter model only if the validation logic fails.

### Real-World RAG & Agentic Context
In **Real-World RAG**, reliability means handling **"Empty Retrieval"**. If your vector search returns zero results, your chain shouldn't crash. You should have a fallback prompt that says, "I couldn't find specific info in our files, would you like me to check the web instead?"
In **Agentic Systems**, reliability is about **"Loop Guards"**. Agents can get caught in "Infinite Loops" (e.g., Tool A calls Tool B which calls Tool A). LangChain enables you to set a `max_iterations` limit, ensuring your agent fails gracefully instead of burning your entire API budget.

### Did you know?
You can use **"Chain-of-Thought"** as a reliability layer. By asking the model to "Think step-by-step before answering," you force it to check its own logic, which often catches errors that a direct answer would have missed.
