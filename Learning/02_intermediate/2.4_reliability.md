# 2.4 Error Handling & Reliability

### Introduction
In the real world, APIs timeout, models hallucinate, and user inputs are messy. A reliable AI system is one built on the assumption that anything can fail.

### Objective
Master the patterns of "Retries" and "Fallbacks" to build systems that stay online even when the provider goes down.

### Concept
Reliability in LangChain is about **Chain Resiliency**:
- **Exponential Backoff:** Retrying an API call but waiting longer each time to avoid being banned for rate limits.
- **Model Fallbacks:** Automatically switching to a different provider (e.g., OpenAI -> Azure -> Ollama) if the primary one returns an error.
- **Output Validation Fallbacks:** If the model returns invalid JSON, retrying with a different prompt that says "You made a mistake, please try again."

### Example
```python
# A robust chain with multiple levels of defense
gpt4 = ChatOpenAI(model="gpt-4o")
fast_gpt3 = ChatOpenAI(model="gpt-4o-mini")
local_llama = ChatOllama(model="llama3.2") 

# NOTE: In Docker, remember to set base_url="http://ollama:11434"

# Try GPT-4 first, if it fails or hits rate limits, use GPT-3.5, 
# and if THAT fails, use the local Llama model.
robust_model = gpt4.with_fallbacks([fast_gpt3, local_llama])

chain = prompt | robust_model | parser
```
 
  
   
    
     

### 2 Use Cases
1. **Critical Service Hubs:** A customer support bot that must answer simple queries using a local model if the internet connection is lost.
2. **Cost-Aware Systems:** Attempting a task with a cheap model first and "falling back" to a smarter model only if the validation logic fails.

### Real-World RAG & Agentic Context
In **Real-World RAG**, reliability means handling **"Empty Retrieval"**. If your vector search returns zero results, your chain shouldn't crash. You should have a fallback prompt that says, "I couldn't find specific info in our files, would you like me to check the web instead?"
In **Agentic Systems**, reliability is about **"Loop Guards"**. Agents can get caught in "Infinite Loops" (e.g., Tool A calls Tool B which calls Tool A). LangChain enables you to set a `max_iterations` limit, ensuring your agent fails gracefully instead of burning your entire API budget.

### Did you know?
You can use **"Chain-of-Thought"** as a reliability layer. By asking the model to "Think step-by-step before answering," you force it to check its own logic, which often catches errors that a direct answer would have missed.
