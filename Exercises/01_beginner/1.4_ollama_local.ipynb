{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d3d62c-0aed-4640-ba0b-e5c269f24d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e21b7a-397e-445b-bb53-9c861d81b3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Local Large Language Model (LLM) orchestration refers to the process of deploying and managing multiple LLMs on a single machine or cluster, rather than relying on cloud-based services. Here are some advantages of local LLM orchestration:\\n\\n1. **Improved Performance**: By having all the LLMs in close proximity, you can reduce latency and improve performance. This is especially important for applications that require real-time responses.\\n2. **Data Privacy and Security**: Storing sensitive data locally ensures that it remains within your organization's control, reducing the risk of data breaches or unauthorized access.\\n3. **Reduced Dependence on Cloud Services**: By not relying on cloud services, you can avoid potential outages, downtime, or changes to pricing models that may impact your application.\\n4. **Increased Control and Customization**: With local LLM orchestration, you have more control over the deployment, configuration, and maintenance of your LLMs, allowing for greater customization and optimization.\\n5. **Lower Latency for Real-time Applications**: For applications that require real-time responses, such as chatbots or virtual assistants, local LLM orchestration can provide faster response times due to reduced latency.\\n6. **Better Integration with On-Premises Infrastructure**: Local LLM orchestration allows you to integrate your LLMs more seamlessly with your existing on-premises infrastructure, reducing the need for additional networking or connectivity overhead.\\n7. **Reduced egress and ingress costs**: By storing data locally, you can reduce the amount of data being transferred in and out of the cloud, resulting in lower egress and ingress costs.\\n8. **Improved Collaboration and Development**: With local LLM orchestration, team members can work together more easily, as they don't need to rely on cloud-based services or worry about latency issues.\\n9. **Reduced Risk of Cloud Service Disruptions**: By having your LLMs locally deployed, you are less vulnerable to disruptions caused by cloud service outages or changes in pricing models.\\n10. **Better Compliance with Regulations**: Local LLM orchestration can help organizations comply with regulations that require sensitive data to be stored within the organization's control.\\n\\nHowever, it's worth noting that local LLM orchestration also has some challenges and limitations, such as:\\n\\n* Higher upfront costs for hardware and infrastructure\\n* Increased complexity in managing multiple LLMs\\n* Potential scalability issues if not properly designed\\n\\nUltimately, the decision to use local LLM orchestration depends on your specific use case, requirements, and priorities.\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-30T07:59:57.95301029Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24092973405, 'load_duration': 2617957259, 'prompt_eval_count': 36, 'prompt_eval_duration': 417794311, 'eval_count': 512, 'eval_duration': 20604514645, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None}, id='run--7ca64e79-d896-4436-babf-9e0676b98993-0', usage_metadata={'input_tokens': 36, 'output_tokens': 512, 'total_tokens': 548})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect using the service name 'ollama' defined in docker-compose.yml\n",
    "llm = ChatOllama(\n",
    "    model=os.getenv(\"OLLAMA_MODEL\", \"llama3.2\"),\n",
    "    base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://ollama:11434\"),\n",
    "    temperature=0\n",
    ")\n",
    "# Test the connection\n",
    "llm.invoke(\"What are the advantages of local LLM orchestration?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fddcb46-ddd5-4cd8-b28f-646de322e6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
