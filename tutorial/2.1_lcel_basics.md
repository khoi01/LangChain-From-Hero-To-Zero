# 2.1 LangChain Expression Language (LCEL)

### Introduction
LCEL is the "engine" that powers all modern LangChain workflows. It moves away from imperative Python code to a declarative, functional style that is easier to scale and maintain.

### Objective
Learn the fundamentals of the pipe operator (`|`) and how it transforms simple functions into powerful, traceable pipelines.

### Concept
In LCEL, objects are "Runnables". When you pipe them:
- **Streaming:** Data flows through the pipe as soon as it's available.
- **Parallelism:** You can run multiple lookups simultaneously.
- **Async:** Built-in support for asynchronous execution for high-performance applications.

### Example
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Summarize this: {context}")
model = ChatOpenAI(model="gpt-4o")
parser = StrOutputParser()

# The "Standard" RAG Chain structure
chain = prompt | model | parser

result = chain.invoke({"context": "LangChain is a framework for LLM orchestration."})
print(result)
```

### 2 Use Cases
1. **Multi-Source Retrieval:** Fetching data from a PDF and a SQL database in parallel, then passing both to an LLM.
2. **Post-Processing Filters:** Piping an LLM's answer into a "Safety Guard" model to ensure it doesn't contain sensitive info.

### Real-World RAG & Agentic Context
In **Real-World RAG**, LCEL is how you build **"Self-RAG"**. You can pipe the retriever into a "Grader" model, then pipe the result into a "Generator" model only if the grading is high enough. LCEL makes these conditional paths readable rather than a mess of `if/else` statements.
In **Agentic Systems**, LCEL is the backbone of **"Reasoning Loops"**. An agent isn't just one call; it's a sequence of `Prompt -> LLM -> Tool Call -> Observation -> Prompt`. LCEL allows you to define this sequence once and let LangChain handle the complex state management between each pipe.

### Did you know?
When you use LCEL, you automatically get **LangSmith Tracing**. This means every single "pipe" in your chain is logged with its exact input and output, making it child's play to debug why a RAG system retrieved the wrong document or why an agent got stuck.
