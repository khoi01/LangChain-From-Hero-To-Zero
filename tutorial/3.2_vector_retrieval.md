# 3.2 Vector Stores & Retrieval

### Introduction
To build a RAG system, you need more than just a list of files; you need a way to search those files by "meaning". Vector Databases like ChromaDB turn words into geometry, where similar ideas are physically close to each other.

### Objective
Learn how to create a vector store, choose an embedding model, and turn a static database into a dynamic "Retriever".

### Concept
1. **Embedding Models:** These convert text into a list of numbers (vectors). OpenAI's `text-embedding-3-small` is a popular, cost-effective choice.
2. **Dimension:** The length of the vector. More dimensions usually mean better "nuance" but slower search.
3. **Similarity Metrics:** How the DB "measures" distance (Cosine Similarity is the standard for text).

### Example
```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# Create the vector store from text snippets
vectorstore = Chroma.from_texts(
    ["The CEO is Alice", "The office is in New York"],
    embedding=OpenAIEmbeddings()
)

# Convert the static store into a LangChain 'Retriever'
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
docs = retriever.invoke("Who leads the company?")
print(docs[0].page_content) # "The CEO is Alice"
```

### 2 Use Cases
1. **Semantic FAQ Bot:** Finding the answer to "How do I reset my password?" even if the user asks "I forgot my sign-in secret."
2. **Document Clustering:** Automatically grouping similar legal contracts together based on their mathematical "meaning" in the vector space.

### Real-World RAG & Agentic Context
In **Real-World RAG**, vector stores are just the beginning. Advanced systems use **"Metadata Filtering"**. For example, your RAG system might retrieve snippets only from documents dated "2024" or only those with "High Importance". This drastically reduces "noise" in the RAG pipeline.
In **Agentic Systems**, a vector store is a **"Memory Retrieval"** tool. An agent might have access to a "Company Archive" vector store. When asked about history, it pulls the relevant historical context into its brain before responding. This is how agents maintain large-scale knowledge without exceeding their token limits.

### Did you know?
Vector search can perfectly understand the relationship between "King" and "Queen" vs "Man" and "Woman". This mathematical relationship allows the AI to "reason" across concepts that never share any identical keywords.
